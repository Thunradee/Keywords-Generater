{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_uri = \"somelinks\".format(<DB_NAME>)\n",
    "client = MongoClient(mongo_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database: TechVault\n",
    "db = client[<DB_NAME>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collection: blogs\n",
    "collection = db[<COLLECTION_NAME>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"Machine Learning \",\n",
    "\"Supervised Learning\",\n",
    "\"Unsupervised Learning\",\n",
    "\"Multilabel Classification\",\n",
    "\"Clustering\",\n",
    "\"K-Means\",\n",
    "\"DBSCAN\",\n",
    "\"Hierarchical Clustering\",\n",
    "\"Deep Learning\",\n",
    "\"Data Mining\",\n",
    "\"Linear regression\",\n",
    "\"Logistic regression\",\n",
    "\"SVM\",\n",
    "\"Natural Language Processing\",\n",
    "\"Computer Vision\",\n",
    "\"KNN\",\n",
    "\"Random forest\",\n",
    "\"Decision Tree\",\n",
    "\"Regularization\",\n",
    "\"Ensemble Learning\",\n",
    "\"Gradient Boosting\",\n",
    "\"Feature Selection\",\n",
    "\"Reinforcement Learning\",\n",
    "\"Virtual Reality\",\n",
    "\"Augmented reality\",\n",
    "\"Autonomous driving\",\n",
    "\"Optics\",\n",
    "\"Biology\",\n",
    "\"C++\",\n",
    "\"Java\",\n",
    "\"Python\",\n",
    "\"React JS\",\n",
    "\"Computer Networks\",\n",
    "\"Frontend\",\n",
    "\"Backend\",\n",
    "\"High Scalability \",\n",
    "\"Cloud computing\",\n",
    "\"Parallel Computing\",\n",
    "\"CUDA\",\n",
    "\"Distributed Systems\",\n",
    "\"Apache ZooKeeper\",\n",
    "\"Streaming analytics\",\n",
    "\"Model Selection\",\n",
    "\"Model Evaluation\",\n",
    "\"Apache Kafka\",\n",
    "\"HDFS\",\n",
    "\"Amazon S3\",\n",
    "\"Pub-Sub\",\n",
    "\"Leader Election\",\n",
    "\"Clock Synchronization\",\n",
    "\"Graphs\",\n",
    "\"Information Retrieval\",\n",
    "\"SQL\",\n",
    "\"Graph Database\",\n",
    "\"Database Management\",\n",
    "\"Storage\",\n",
    "\"Memory\",\n",
    "\"Garbage Collection\",\n",
    "\"Map-Reduce\",\n",
    "\"Network Protocols\",\n",
    "\"Cyber Security\",\n",
    "\"Assembly Language\",\n",
    "\"Computational Complexity Theory\",\n",
    "\"Computer Architecture\",\n",
    "\"Human-Computer Interface\",\n",
    "\"Data Structures\",\n",
    "\"Discrete Mathematics\",\n",
    "\"Hacking\",\n",
    "\"Quantum Computing\",\n",
    "\"Robotics\",\n",
    "\"Engineering Practices\",\n",
    "\"Software Tools\",\n",
    "\"Mathematical Logic\",\n",
    "\"Graph Theory\",\n",
    "\"Computational Geometry\",\n",
    "\"Algorithms\",\n",
    "\"Compilers\",\n",
    "\"Distributed Computing\",\n",
    "\"Software Engineering\",\n",
    "\"Bioinformatics\",\n",
    "\"Computational Chemistry\",\n",
    "\"Computational Neuroscience\",\n",
    "\"Computational physics\",\n",
    "\"Numerical algorithms\",\n",
    "\"JavaScript\",\n",
    "\"HTML\",\n",
    "\"Web Development\",\n",
    "\"App Development\",\n",
    "\"CSS\",\n",
    "\"PHP\",\n",
    "\"BlockChain\",\n",
    "\"Hardware\",\n",
    "\"VLSI\",\n",
    "\"Cluster Computing\",\n",
    "\"Kubernetes\",\n",
    "\"Go\",\n",
    "\"File Systems\",\n",
    "\"Statistics\",\n",
    "\"Optimization\",\n",
    "\"Knowledge Graph\",\n",
    "\"RNN\",\n",
    "\"CNN\",\n",
    "\"Physical Design\",\n",
    "\"Memory management\",\n",
    "\"PCA\",\n",
    "\"LDA\",\n",
    "\"Feature Engineering\",\n",
    "\"Data manipulation\",\n",
    "\"ACID\",\n",
    "\"BASE\",\n",
    "\"Consistency\",\n",
    "\"Disaster recovery\",\n",
    "\"Replication\",\n",
    "\"Fault tolerance\",\n",
    "\"Deployment\",\n",
    "\"Processors\",\n",
    "\"Multi-Threading\",\n",
    "\"Queue\",\n",
    "\"Stack\",\n",
    "\"Dynamic Programming\",\n",
    "\"Graph Traversal\",\n",
    "\"Devices\",\n",
    "\"Data analysis\",\n",
    "\"Probability\",\n",
    "\"Mathematics\",\n",
    "\"Genomics\",\n",
    "\"Data Infrastructure\",\n",
    "\"Software Principles and Practices\",\n",
    "\"Image Processing\",\n",
    "\"Audio Processing\",\n",
    "\"Signal Processing\",\n",
    "\"Pattern Recognition\",\n",
    "\"Computation and Language\",\n",
    "\"Artificial Intelligence\",\n",
    "\"Computation and Language\",\n",
    "\"Computational Complexity\",\n",
    "\"Computational Engineering\",\n",
    "\"Finance, and Science\",\n",
    "\"Computational Geometry\",\n",
    "\"Computer Science and Game Theory\",\n",
    "\"Computer Vision and Pattern Recognition\",\n",
    "\"Computers and Society\",\n",
    "\"Cryptography and Security\",\n",
    "\"Data Structures and Algorithms\",\n",
    "\"Databases; Digital Libraries\",\n",
    "\"Discrete Mathematics\",\n",
    "\"Distributed, Parallel, and Cluster Computing\",\n",
    "\"Emerging Technologies\",\n",
    "\"Formal Languages and Automata Theory\",\n",
    "\"General Literature\",\n",
    "\"Graphics\",\n",
    "\"Human-Computer Interaction\",\n",
    "\"Information Theory\",\n",
    "\"Logic in Computer Science\",\n",
    "\"Mathematical Software\",\n",
    "\"Multiagent Systems\",\n",
    "\"Multimedia\",\n",
    "\"Networking and Internet Architecture\",\n",
    "\"Neural and Evolutionary Computing\",\n",
    "\"Numerical Analysis\",\n",
    "\"Operating Systems\",\n",
    "\"Performance\",\n",
    "\"Programming Languages\",\n",
    "\"Social and Information Networks\",\n",
    "\"Software Engineering\",\n",
    "\"Sound\",\n",
    "\"Symbolic Computation\",\n",
    "\"Systems and Control\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with one paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  In this paper, we introduce the on-line Viterbi algorithm for decoding hidden\\nMarkov models (HMMs) in much smaller than linear space. Our analysis on\\ntwo-state HMMs suggests that the expected maximum memory used to decode\\nsequence of length $n$ with $m$-state HMM can be as low as $\\\\Theta(m\\\\log n)$,\\nwithout a significant slow-down compared to the classical Viterbi algorithm.\\nClassical Viterbi algorithm requires $O(mn)$ space, which is impractical for\\nanalysis of long DNA sequences (such as complete human genome chromosomes) and\\nfor continuous data streams. We also experimentally demonstrate the performance\\nof the on-line Viterbi algorithm on a simple HMM for gene finding on both\\nsimulated and real DNA sequences.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One instance in blogs collection\n",
    "blogs = collection.find({\"link\":\"https://arxiv.org/pdf/0704.0062\"})\n",
    "blog = list(blogs)[0]\n",
    "abstract = blog['abstract']\n",
    "abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' in this paper we introduce the on line viterbi algorithm for decoding hidden markov models hmms in much smaller than linear space our analysis on two state hmms suggests that the expected maximum memory used to decode sequence of length n with m state hmm can be as low as thetamlog n without a significant slow down compared to the classical viterbi algorithm classical viterbi algorithm requires omn space which is impractical for analysis of long dna sequences such as complete human genome chromosomes and for continuous data streams we also experimentally demonstrate the performance of the on line viterbi algorithm on a simple hmm for gene finding on both simulated and real dna sequences '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Funtion to remove HTML tags\n",
    "def remove_html_tags(text):\n",
    "    return BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "\n",
    "# Function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    # define the pattern to keep only letters and numbers\n",
    "    pat = r'[^a-zA-Z0-9_\\s]+'\n",
    "    return re.sub(pat, '', text)\n",
    "\n",
    "\n",
    "# Function to remove special characters and escape characters\n",
    "def remove_extra_whitespace_esc(text):\n",
    "    #pattern = r'^\\s+$|\\s+$'\n",
    "    pattern = r'^\\s*|\\s\\s*'\n",
    "    return re.sub(pattern, ' ', text).strip()\n",
    "\n",
    "modified_abstract = abstract.replace('-', ' ')  # Replace '-' with white space\n",
    "modified_abstract = remove_html_tags(modified_abstract)  # Remove HTML tegs\n",
    "modified_abstract = remove_special_characters(modified_abstract)  # Remove special characters\n",
    "modified_abstract = remove_extra_whitespace_esc(modified_abstract)  # Remove white space and escape charecters\n",
    "modified_abstract = ' ' + modified_abstract + ' '  # Add white space to the beginning and the end of text\n",
    "modified_abstract = modified_abstract.lower()\n",
    "modified_abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keywords for this paper should be\n",
    " - on-line Viterbi algorithm\n",
    " - Classical Viterbi algorithm\n",
    " - hidden Markov models/HMMs\n",
    " - DNA Squences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Memory': 1, 'Performance': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {}\n",
    "\n",
    "# Count occurance of keywords in text\n",
    "for word in keywords:\n",
    "    oc = modified_abstract.count(' ' + word.lower() + ' ')\n",
    "    if oc > 0:\n",
    "        d[word] = oc\n",
    "        \n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "Source: https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea\n",
    "### 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm',\n",
       " 'analysis',\n",
       " 'chromosomes',\n",
       " 'classical',\n",
       " 'compared',\n",
       " 'complete',\n",
       " 'continuous',\n",
       " 'data',\n",
       " 'decode',\n",
       " 'decoding',\n",
       " 'demonstrate',\n",
       " 'dna',\n",
       " 'expected',\n",
       " 'experimentally',\n",
       " 'finding',\n",
       " 'gene',\n",
       " 'genome',\n",
       " 'hidden',\n",
       " 'hmm',\n",
       " 'hmms',\n",
       " 'human',\n",
       " 'impractical',\n",
       " 'introduce',\n",
       " 'length',\n",
       " 'line',\n",
       " 'linear',\n",
       " 'long',\n",
       " 'low',\n",
       " 'markov',\n",
       " 'maximum',\n",
       " 'memory',\n",
       " 'models',\n",
       " 'omn',\n",
       " 'paper',\n",
       " 'performance',\n",
       " 'real',\n",
       " 'requires',\n",
       " 'sequence',\n",
       " 'sequences',\n",
       " 'significant',\n",
       " 'simple',\n",
       " 'simulated',\n",
       " 'slow',\n",
       " 'smaller',\n",
       " 'space',\n",
       " 'state',\n",
       " 'streams',\n",
       " 'suggests',\n",
       " 'thetamlog',\n",
       " 'used',\n",
       " 'viterbi']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_gram_range = (1, 1)  # Size of phases: include 1 word\n",
    "stop_words = \"english\"\n",
    "\n",
    "# Extract candidate words/phrases\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([modified_abstract])\n",
    "candidates = count.get_feature_names()\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install needed libraries\n",
    "# Install Pytorch\n",
    "# pip install torch torchvision\n",
    "# Install SentenceTransformer\n",
    "# pip install sentence-transformers\n",
    "\n",
    "# Generate the BERT embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([modified_abstract])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decoding', 'markov', 'genome', 'dna', 'algorithm']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Cosine Similarity: genrate keywords that most similar to the document\n",
    "# Problems: leck of diversity among top picked keywords\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "top_n = 5 # Top 5 keywords\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# Using Max Sum Similarity: \n",
    "# maximize the candidate similarity to the document whilst minimizing the similarity between candidates\n",
    "# Higher 'nr_candidates' gives more diversity to generated keywords but the generated keywords may not well represent\n",
    "# document. The author recommends less than 20%\n",
    "def max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n, nr_candidates):\n",
    "    # Calculate distances and extract keywords\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # Get top_n words as candidates based on cosine similarity\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Calculate the combination of words that are the least similar to each other\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Maximal Marginal Relevance: \n",
    "# Select new candidates that are both similar to the document and not similar to the already selected\n",
    "\n",
    "def mmr(doc_embedding, word_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['space', 'slow', 'impractical', 'classical', 'dna']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm', 'real', 'models', 'streams', 'smaller']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm classical',\n",
       " 'algorithm decoding',\n",
       " 'algorithm requires',\n",
       " 'algorithm simple',\n",
       " 'analysis long',\n",
       " 'analysis state',\n",
       " 'chromosomes continuous',\n",
       " 'classical viterbi',\n",
       " 'compared classical',\n",
       " 'complete human',\n",
       " 'continuous data',\n",
       " 'data streams',\n",
       " 'decode sequence',\n",
       " 'decoding hidden',\n",
       " 'demonstrate performance',\n",
       " 'dna sequences',\n",
       " 'expected maximum',\n",
       " 'experimentally demonstrate',\n",
       " 'finding simulated',\n",
       " 'gene finding',\n",
       " 'genome chromosomes',\n",
       " 'hidden markov',\n",
       " 'hmm gene',\n",
       " 'hmm low',\n",
       " 'hmms smaller',\n",
       " 'hmms suggests',\n",
       " 'human genome',\n",
       " 'impractical analysis',\n",
       " 'introduce line',\n",
       " 'length state',\n",
       " 'line viterbi',\n",
       " 'linear space',\n",
       " 'long dna',\n",
       " 'low thetamlog',\n",
       " 'markov models',\n",
       " 'maximum memory',\n",
       " 'memory used',\n",
       " 'models hmms',\n",
       " 'omn space',\n",
       " 'paper introduce',\n",
       " 'performance line',\n",
       " 'real dna',\n",
       " 'requires omn',\n",
       " 'sequence length',\n",
       " 'sequences complete',\n",
       " 'significant slow',\n",
       " 'simple hmm',\n",
       " 'simulated real',\n",
       " 'slow compared',\n",
       " 'smaller linear',\n",
       " 'space analysis',\n",
       " 'space impractical',\n",
       " 'state hmm',\n",
       " 'state hmms',\n",
       " 'streams experimentally',\n",
       " 'suggests expected',\n",
       " 'thetamlog significant',\n",
       " 'used decode',\n",
       " 'viterbi algorithm']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_gram_range = (2, 2)  # Size of phases is 2 words\n",
    "stop_words = \"english\"\n",
    "\n",
    "# Extract candidate words/phrases\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([modified_abstract])\n",
    "candidates = count.get_feature_names()\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the BERT embeddings\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([modified_abstract])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['genome chromosomes',\n",
       " 'viterbi algorithm',\n",
       " 'markov models',\n",
       " 'hidden markov',\n",
       " 'algorithm decoding']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Cosine Similarity: genrate keywords that most similar to the document\n",
    "# Problems: leck of diversity among top picked keywords\n",
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['space analysis',\n",
       " 'algorithm simple',\n",
       " 'space impractical',\n",
       " 'genome chromosomes',\n",
       " 'markov models']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr_candidates: 5\n",
      "['genome chromosomes', 'viterbi algorithm', 'markov models', 'hidden markov', 'algorithm decoding']\n",
      "\n",
      "nr_candidates: 10\n",
      "['space impractical', 'long dna', 'genome chromosomes', 'markov models', 'algorithm decoding']\n",
      "\n",
      "nr_candidates: 15\n",
      "['algorithm simple', 'space impractical', 'genome chromosomes', 'markov models', 'algorithm decoding']\n",
      "\n",
      "nr_candidates: 20\n",
      "['space analysis', 'algorithm simple', 'space impractical', 'genome chromosomes', 'markov models']\n",
      "\n",
      "nr_candidates: 25\n",
      "['space analysis', 'algorithm simple', 'space impractical', 'genome chromosomes', 'markov models']\n",
      "\n",
      "nr_candidates: 30\n",
      "['space analysis', 'algorithm simple', 'space impractical', 'genome chromosomes', 'markov models']\n",
      "\n",
      "nr_candidates: 35\n",
      "['space analysis', 'algorithm simple', 'space impractical', 'genome chromosomes', 'markov models']\n",
      "\n",
      "nr_candidates: 40\n",
      "['space analysis', 'algorithm simple', 'space impractical', 'genome chromosomes', 'markov models']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try different 'nr_candidates' values\n",
    "for n in range(5, 41, 5):\n",
    "    print(\"nr_candidates: {}\".format(n))\n",
    "    print(max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n, n))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm decoding',\n",
       " 'complete human',\n",
       " 'hmms smaller',\n",
       " 'space analysis',\n",
       " 'introduce line']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diversity: 5\n",
      "['algorithm decoding', 'complete human', 'hmms smaller', 'space analysis', 'paper introduce']\n",
      "\n",
      "diversity: 10\n",
      "['algorithm decoding', 'complete human', 'hmms smaller', 'space analysis', 'introduce line']\n",
      "\n",
      "diversity: 15\n",
      "['algorithm decoding', 'complete human', 'hmms smaller', 'space analysis', 'introduce line']\n",
      "\n",
      "diversity: 20\n",
      "['algorithm decoding', 'complete human', 'hmms smaller', 'space analysis', 'introduce line']\n",
      "\n",
      "diversity: 25\n",
      "['algorithm decoding', 'complete human', 'hmms smaller', 'space analysis', 'introduce line']\n",
      "\n",
      "diversity: 30\n",
      "['algorithm decoding', 'complete human', 'hmms smaller', 'space analysis', 'introduce line']\n",
      "\n",
      "diversity: 35\n",
      "['algorithm decoding', 'complete human', 'hmms smaller', 'space analysis', 'introduce line']\n",
      "\n",
      "diversity: 40\n",
      "['algorithm decoding', 'complete human', 'hmms smaller', 'space analysis', 'introduce line']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try different 'diversity' values\n",
    "for n in range(5, 41, 5):\n",
    "    print(\"diversity: {}\".format(n))\n",
    "    print(mmr(doc_embedding, candidate_embeddings, candidates, top_n, n))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm classical viterbi',\n",
       " 'algorithm decoding hidden',\n",
       " 'algorithm requires omn',\n",
       " 'algorithm simple hmm',\n",
       " 'analysis long dna',\n",
       " 'analysis state hmms',\n",
       " 'chromosomes continuous data',\n",
       " 'classical viterbi algorithm',\n",
       " 'compared classical viterbi',\n",
       " 'complete human genome',\n",
       " 'continuous data streams',\n",
       " 'data streams experimentally',\n",
       " 'decode sequence length',\n",
       " 'decoding hidden markov',\n",
       " 'demonstrate performance line',\n",
       " 'dna sequences complete',\n",
       " 'expected maximum memory',\n",
       " 'experimentally demonstrate performance',\n",
       " 'finding simulated real',\n",
       " 'gene finding simulated',\n",
       " 'genome chromosomes continuous',\n",
       " 'hidden markov models',\n",
       " 'hmm gene finding',\n",
       " 'hmm low thetamlog',\n",
       " 'hmms smaller linear',\n",
       " 'hmms suggests expected',\n",
       " 'human genome chromosomes',\n",
       " 'impractical analysis long',\n",
       " 'introduce line viterbi',\n",
       " 'length state hmm',\n",
       " 'line viterbi algorithm',\n",
       " 'linear space analysis',\n",
       " 'long dna sequences',\n",
       " 'low thetamlog significant',\n",
       " 'markov models hmms',\n",
       " 'maximum memory used',\n",
       " 'memory used decode',\n",
       " 'models hmms smaller',\n",
       " 'omn space impractical',\n",
       " 'paper introduce line',\n",
       " 'performance line viterbi',\n",
       " 'real dna sequences',\n",
       " 'requires omn space',\n",
       " 'sequence length state',\n",
       " 'sequences complete human',\n",
       " 'significant slow compared',\n",
       " 'simple hmm gene',\n",
       " 'simulated real dna',\n",
       " 'slow compared classical',\n",
       " 'smaller linear space',\n",
       " 'space analysis state',\n",
       " 'space impractical analysis',\n",
       " 'state hmm low',\n",
       " 'state hmms suggests',\n",
       " 'streams experimentally demonstrate',\n",
       " 'suggests expected maximum',\n",
       " 'thetamlog significant slow',\n",
       " 'used decode sequence',\n",
       " 'viterbi algorithm classical',\n",
       " 'viterbi algorithm decoding',\n",
       " 'viterbi algorithm requires',\n",
       " 'viterbi algorithm simple']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_range = (3, 3)  # phases include 3 words\n",
    "stop_words = \"english\"\n",
    "\n",
    "# Extract candidate words/phrases\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([modified_abstract])\n",
    "candidates = count.get_feature_names()\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([modified_abstract])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['analysis long dna',\n",
       " 'hidden markov models',\n",
       " 'viterbi algorithm decoding',\n",
       " 'algorithm decoding hidden',\n",
       " 'decoding hidden markov']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['complete human genome',\n",
       " 'space impractical analysis',\n",
       " 'human genome chromosomes',\n",
       " 'viterbi algorithm simple',\n",
       " 'algorithm decoding hidden']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decoding hidden markov',\n",
       " 'sequences complete human',\n",
       " 'state hmm low',\n",
       " 'introduce line viterbi',\n",
       " 'experimentally demonstrate performance']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
